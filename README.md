# üç∫ BEES ‚Äì Desafio T√©cnico de Engenharia de Dados

## üìò Vis√£o Geral do Projeto

Este projeto foi desenvolvido como parte de um desafio t√©cnico para a √°rea de Engenharia de Dados na BEES. Seu objetivo √© demonstrar a capacidade de construir uma pipeline de dados robusta, modular e escal√°vel utilizando tecnologias amplamente adotadas no mercado.

A pipeline implementa um fluxo completo de dados que abrange:

- Extra√ß√£o de dados p√∫blicos a partir da **API Open Brewery DB**;
- Armazenamento dos dados brutos (camada **Bronze**) em um data lake simulado com o **MinIO**;
- Transforma√ß√£o e limpeza dos dados com **Apache Spark** e persist√™ncia como **tabelas Delta** (camada **Silver**);
- Agrega√ß√µes anal√≠ticas para cria√ß√£o de uma vis√£o de neg√≥cio (camada **Gold**), tamb√©m armazenada em formato Delta;
- Valida√ß√µes autom√°ticas em cada etapa para garantir qualidade, formato e consist√™ncia dos dados.

Al√©m disso, a solu√ß√£o √© **totalmente containerizada** com **Docker Compose**, permitindo f√°cil execu√ß√£o local com componentes como Airflow, Spark, MinIO e LocalStack.

O projeto segue o padr√£o **Medallion Architecture** e foi desenvolvido com foco em:

- Clareza de c√≥digo e modulariza√ß√£o
- Observabilidade e reprocessamento por data
- Facilidade de testes unit√°rios
- Expansibilidade futura para ferramentas de Data Quality (ex: Great Expectations) e de consulta (ex: Trino)

Este projeto implementa um pipeline de ingest√£o, transforma√ß√£o e agrega√ß√£o de dados da [Open Brewery DB](https://www.openbrewerydb.org/), utilizando **Apache Airflow**, **Spark com Delta Lake**, **MinIO** como data lake e **LocalStack** para simula√ß√£o de servi√ßos AWS (SSM). A arquitetura segue o padr√£o **Medallion** (Bronze, Silver, Gold).

---

## üéØ Objetivos Atendidos

| Requisito                                             | Status |
|-------------------------------------------------------|--------|
| Consumo da API Open Brewery DB                        | ‚úÖ     |
| Orquestra√ß√£o com ferramenta (Airflow)                 | ‚úÖ     |
| Transforma√ß√µes com particionamento por localiza√ß√£o    | ‚úÖ     |
| Arquitetura Medallion (Bronze, Silver, Gold)          | ‚úÖ     |
| Docker e Docker Compose                               | ‚úÖ     |
| Testes unit√°rios                                      | ‚úÖ     |
| Valida√ß√µes autom√°ticas ap√≥s cada etapa                | ‚úÖ     |
| Modulariza√ß√£o e qualidade de c√≥digo                   | ‚úÖ     |
| Explica√ß√£o t√©cnica e execu√ß√£o documentada             | ‚úÖ     |

---

## üß± Arquitetura e Camadas

### üîπ Bronze (Raw)
- Extra√ß√£o da API Open Brewery DB
- Salvamento de JSONs no MinIO: `s3://bronze/breweries_raw_<YYYYMMDD>.json`

### ‚ö™ Silver (Curated)
- Transforma√ß√£o com Spark (Delta Lake)
- Particionado por `state`
- Salvo em: `s3://silver/date=<YYYYMMDD>/`

### üü° Gold (Analytics)
- Agrega√ß√£o: contagem por `brewery_type` e `state`
- Salvo em: `s3://gold/breweries_summary/date=<YYYYMMDD>/`

---

## ‚öôÔ∏è Execu√ß√£o local

### 1. Pr√©-requisitos
- Docker
- Docker Compose

### 2. Subir os containers

```bash
docker network create airflow-net
docker-compose build


docker-compose run --rm airflow-webserver airflow db init
docker-compose run --rm airflow-webserver airflow users create \
    --username admin --password admin \
    --firstname Admin --lastname User \
    --role Admin --email admin@example.com

docker-compose up -d

```

> Aguarde a cria√ß√£o da infraestrutura: Airflow, Redis, Postgres, MinIO, LocalStack.

### 3. Acessar interfaces

# Acesse o MinIO Console e valide se os buckets bronze, silver e gold estao presentes, caso nao esteja os crie com as configura√ß√µes b√°sicas.

# Acesse o Airflow e ative a dag.

| Servi√ßo       | URL                         | Login  | Senha      |
|---------------|------------------------------|--------|------------|
| Airflow       | http://localhost:8080        | admin  | admin   |
| MinIO Console | http://localhost:9001        | admin  | admin123   |

---

## üóÇ Estrutura do Projeto

```bash
bees_breweries_pipeline/
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îú‚îÄ‚îÄ breweries_pipeline.py       # DAG principal
‚îÇ   ‚îî‚îÄ‚îÄ src/
‚îÇ       ‚îú‚îÄ‚îÄ extract.py              # Extra√ß√£o da API
‚îÇ       ‚îú‚îÄ‚îÄ transform.py            # Bronze ‚ûù Silver
‚îÇ       ‚îú‚îÄ‚îÄ aggregate.py            # Silver ‚ûù Gold
‚îÇ       ‚îú‚îÄ‚îÄ validate.py             # Valida√ß√µes por camada
‚îÇ       ‚îú‚îÄ‚îÄ config.py               # SSM fake + paths
‚îÇ       ‚îú‚îÄ‚îÄ s3_utils.py             # Opera√ß√µes com MinIO
‚îÇ       ‚îî‚îÄ‚îÄ spark_utils.py          # Sess√£o Spark
‚îú‚îÄ‚îÄ tests/                          # Testes unit√°rios
‚îú‚îÄ‚îÄ scripts/                        # Inicializa√ß√£o do LocalStack
‚îú‚îÄ‚îÄ .env                            # Config do Airflow
‚îú‚îÄ‚îÄ docker-compose.yaml             # Infraestrutura
‚îî‚îÄ‚îÄ readme.md
```

---

## üîÑ DAG: `breweries_etl_pipeline`

Orquestrada com Airflow. Executa:

```text
extract_breweries
‚Üí validate_bronze_data
‚Üí transform_to_silver
‚Üí validate_silver_data
‚Üí aggregate_to_gold
‚Üí validate_gold_data
‚Üí log_metrics
```

- Agendamento: `@daily`
- Retry autom√°tico: 5 vezes
- Envio de e-mail em falhas (simulado via console)

---

## ‚úÖ Valida√ß√µes de Qualidade

Implementadas como **steps independentes**:

| Camada   | Valida√ß√µes                                                                           |
|----------|---------------------------------------------------------------------------------------|
| Bronze   | Arquivo existe no S3, schema m√≠nimo, colunas obrigat√≥rias, aus√™ncia de duplicatas    |
| Silver   | Dados n√£o nulos, colunas obrigat√≥rias, IDs √∫nicos                                    |
| Gold     | Campos esperados, `count > 0`, sem duplicatas por (`state`, `brewery_type`)          |

---

## üß™ Executar Testes Unit√°rios

Com o Airflow rodando:

```bash
docker-compose exec airflow-webserver pytest dags/tests/
```

---

## üí° Decis√µes T√©cnicas e considera√ß√µes

- **MinIO** foi utilizado para simular um data lake compat√≠vel com S3, permitindo testes locais.
- **Spark** foi escolhido pela escalabilidade e compatibilidade com Delta Lake.
- **LocalStack** permite simula√ß√£o de SSM para carregar dinamicamente os paths dos buckets.
- Todas as etapas s√£o **idempotentes** e podem ser reexecutadas com seguran√ßa por data.
- O uso de um motor de consulta sql como o Trino poderia ser usado para disponibilizar os dados em plataformas de BI/Data Visualization.
- O uso de Great Expectations, embora recomend√°vel, n√£o foi utilizado no projeto.

---

