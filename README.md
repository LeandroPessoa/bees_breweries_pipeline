# ğŸº BEES â€“ Desafio TÃ©cnico de Engenharia de Dados

Este projeto implementa um pipeline de ingestÃ£o, transformaÃ§Ã£o e agregaÃ§Ã£o de dados da [Open Brewery DB](https://www.openbrewerydb.org/), utilizando **Apache Airflow**, **Spark com Delta Lake**, **MinIO** como data lake e **LocalStack** para simulaÃ§Ã£o de serviÃ§os AWS (SSM). A arquitetura segue o padrÃ£o **Medallion** (Bronze, Silver, Gold).

---

## ğŸ¯ Objetivos Atendidos

| Requisito                                             | Status |
|-------------------------------------------------------|--------|
| Consumo da API Open Brewery DB                        | âœ…     |
| OrquestraÃ§Ã£o com ferramenta (Airflow)                 | âœ…     |
| TransformaÃ§Ãµes com particionamento por localizaÃ§Ã£o    | âœ…     |
| Arquitetura Medallion (Bronze, Silver, Gold)          | âœ…     |
| Docker e Docker Compose                               | âœ…     |
| Testes unitÃ¡rios                                      | âœ…     |
| ValidaÃ§Ãµes automÃ¡ticas apÃ³s cada etapa                | âœ…     |
| ModularizaÃ§Ã£o e qualidade de cÃ³digo                   | âœ…     |
| ExplicaÃ§Ã£o tÃ©cnica e execuÃ§Ã£o documentada             | âœ…     |

---

## ğŸ§± Arquitetura e Camadas

### ğŸ”¹ Bronze (Raw)
- ExtraÃ§Ã£o da API Open Brewery DB
- Salvamento de JSONs no MinIO: `s3://bronze/breweries_raw_<YYYYMMDD>.json`

### âšª Silver (Curated)
- TransformaÃ§Ã£o com Spark (Delta Lake)
- Particionado por `state`
- Salvo em: `s3://silver/date=<YYYYMMDD>/`

### ğŸŸ¡ Gold (Analytics)
- AgregaÃ§Ã£o: contagem por `brewery_type` e `state`
- Salvo em: `s3://gold/breweries_summary/date=<YYYYMMDD>/`

---

## âš™ï¸ ExecuÃ§Ã£o local

### 1. PrÃ©-requisitos
- Docker
- Docker Compose

### 2. Subir os containers

```bash
docker network create airflow-net
docker-compose up --build


docker-compose run --rm airflow-webserver airflow db init
docker-compose run --rm airflow-webserver airflow users create \
    --username admin --password admin \
    --firstname Admin --lastname User \
    --role Admin --email admin@example.com

docker-compose up -d

```

> Aguarde a criaÃ§Ã£o da infraestrutura: Airflow, Redis, Postgres, MinIO, LocalStack.

### 3. Acessar interfaces

| ServiÃ§o       | URL                         | Login  | Senha      |
|---------------|------------------------------|--------|------------|
| Airflow       | http://localhost:8080        | admin  | admin   |
| MinIO Console | http://localhost:9001        | admin  | admin123   |

---

## ğŸ—‚ Estrutura do Projeto

```bash
bees_breweries_pipeline/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ breweries_pipeline.py       # DAG principal
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ extract.py              # ExtraÃ§Ã£o da API
â”‚       â”œâ”€â”€ transform.py            # Bronze â Silver
â”‚       â”œâ”€â”€ aggregate.py            # Silver â Gold
â”‚       â”œâ”€â”€ validate.py             # ValidaÃ§Ãµes por camada
â”‚       â”œâ”€â”€ config.py               # SSM fake + paths
â”‚       â”œâ”€â”€ s3_utils.py             # OperaÃ§Ãµes com MinIO
â”‚       â””â”€â”€ spark_utils.py          # SessÃ£o Spark
â”œâ”€â”€ tests/                          # Testes unitÃ¡rios
â”œâ”€â”€ scripts/                        # InicializaÃ§Ã£o do LocalStack
â”œâ”€â”€ .env                            # Config do Airflow
â”œâ”€â”€ docker-compose.yaml             # Infraestrutura
â””â”€â”€ readme.md
```

---

## ğŸ”„ DAG: `breweries_etl_pipeline`

Orquestrada com Airflow. Executa:

```text
extract_breweries
â†’ validate_bronze_data
â†’ transform_to_silver
â†’ validate_silver_data
â†’ aggregate_to_gold
â†’ validate_gold_data
â†’ log_metrics
```

- Agendamento: `@daily`
- Retry automÃ¡tico: 5 vezes
- Envio de e-mail em falhas (simulado via console)

---

## âœ… ValidaÃ§Ãµes de Qualidade

Implementadas como **steps independentes**:

| Camada   | ValidaÃ§Ãµes                                                                           |
|----------|---------------------------------------------------------------------------------------|
| Bronze   | Arquivo existe no S3, schema mÃ­nimo, colunas obrigatÃ³rias, ausÃªncia de duplicatas    |
| Silver   | Dados nÃ£o nulos, colunas obrigatÃ³rias, IDs Ãºnicos                                    |
| Gold     | Campos esperados, `count > 0`, sem duplicatas por (`state`, `brewery_type`)          |

---

## ğŸ§ª Executar Testes UnitÃ¡rios

Com o Airflow rodando:

```bash
docker-compose exec airflow-webserver pytest dags/tests/
```

---

## ğŸ’¡ DecisÃµes TÃ©cnicas e consideraÃ§Ãµes

- **MinIO** foi utilizado para simular um data lake compatÃ­vel com S3, permitindo testes locais.
- **Spark** foi escolhido pela escalabilidade e compatibilidade com Delta Lake.
- **LocalStack** permite simulaÃ§Ã£o de SSM para carregar dinamicamente os paths dos buckets.
- Todas as etapas sÃ£o **idempotentes** e podem ser reexecutadas com seguranÃ§a por data.
- O uso de um motor de consulta sql como o Trino poderia ser usado para disponibilizar os dados em plataformas de BI/Data Visualization.
- O uso de Great Expectations, embora recomendÃ¡vel, nÃ£o foi utilizado no projeto.

---

