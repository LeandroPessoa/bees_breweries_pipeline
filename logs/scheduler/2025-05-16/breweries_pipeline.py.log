[2025-05-16T00:00:59.742+0000] {processor.py:161} INFO - Started process (PID=481) to work on /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:00:59.746+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/breweries_pipeline.py for tasks to queue
[2025-05-16T00:00:59.749+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:00:59.748+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:01:00.872+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/bronze_path = s3a://bronze
[2025-05-16T00:01:00.908+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/silver_path = s3a://silver
[2025-05-16T00:01:00.938+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/gold_path = s3a://gold
[2025-05-16T00:01:00.968+0000] {processor.py:840} INFO - DAG(s) 'breweries_etl_pipeline' retrieved from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:01:01.039+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:01:01.037+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-05-16T00:01:01.086+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:01:01.085+0000] {dag.py:3823} INFO - Setting next_dagrun for breweries_etl_pipeline to 2025-05-16 00:00:00+00:00, run_after=2025-05-17 00:00:00+00:00
[2025-05-16T00:01:21.860+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/breweries_pipeline.py took 22.126 seconds
[2025-05-16T00:01:52.171+0000] {processor.py:161} INFO - Started process (PID=483) to work on /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:01:52.173+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/breweries_pipeline.py for tasks to queue
[2025-05-16T00:01:52.176+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:01:52.174+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:01:55.642+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/bronze_path = s3a://bronze
[2025-05-16T00:01:55.681+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/silver_path = s3a://silver
[2025-05-16T00:01:55.718+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/gold_path = s3a://gold
[2025-05-16T00:01:55.801+0000] {processor.py:840} INFO - DAG(s) 'breweries_etl_pipeline' retrieved from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:01:55.901+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:01:55.899+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-05-16T00:02:02.764+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:02:02.763+0000] {dag.py:3823} INFO - Setting next_dagrun for breweries_etl_pipeline to 2025-05-16 00:00:00+00:00, run_after=2025-05-17 00:00:00+00:00
[2025-05-16T00:02:06.806+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/breweries_pipeline.py took 14.646 seconds
[2025-05-16T00:02:37.138+0000] {processor.py:161} INFO - Started process (PID=485) to work on /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:02:37.140+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/breweries_pipeline.py for tasks to queue
[2025-05-16T00:02:37.141+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:02:37.141+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:02:42.571+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/bronze_path = s3a://bronze
[2025-05-16T00:02:42.604+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/silver_path = s3a://silver
[2025-05-16T00:02:42.644+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/gold_path = s3a://gold
[2025-05-16T00:02:42.677+0000] {processor.py:840} INFO - DAG(s) 'breweries_etl_pipeline' retrieved from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:02:42.767+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:02:42.766+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-05-16T00:02:42.841+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:02:42.840+0000] {dag.py:3823} INFO - Setting next_dagrun for breweries_etl_pipeline to 2025-05-16 00:00:00+00:00, run_after=2025-05-17 00:00:00+00:00
[2025-05-16T00:03:17.437+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/breweries_pipeline.py took 40.304 seconds
[2025-05-16T00:03:47.812+0000] {processor.py:161} INFO - Started process (PID=487) to work on /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:03:47.814+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/breweries_pipeline.py for tasks to queue
[2025-05-16T00:03:47.818+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:03:47.816+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:03:57.149+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/bronze_path = s3a://bronze
[2025-05-16T00:03:57.299+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/silver_path = s3a://silver
[2025-05-16T00:03:57.354+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/gold_path = s3a://gold
[2025-05-16T00:03:57.402+0000] {processor.py:840} INFO - DAG(s) 'breweries_etl_pipeline' retrieved from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:03:57.609+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:03:57.609+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-05-16T00:05:08.788+0000] {processor.py:161} INFO - Started process (PID=489) to work on /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:05:08.791+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/breweries_pipeline.py for tasks to queue
[2025-05-16T00:05:08.794+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:05:08.793+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:05:10.528+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/bronze_path = s3a://bronze
[2025-05-16T00:05:10.566+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/silver_path = s3a://silver
[2025-05-16T00:05:10.601+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/gold_path = s3a://gold
[2025-05-16T00:05:10.634+0000] {processor.py:840} INFO - DAG(s) 'breweries_etl_pipeline' retrieved from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:05:10.727+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:05:10.726+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-05-16T00:05:31.529+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:05:31.528+0000] {dag.py:3823} INFO - Setting next_dagrun for breweries_etl_pipeline to 2025-05-16 00:00:00+00:00, run_after=2025-05-17 00:00:00+00:00
[2025-05-16T00:05:31.599+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/breweries_pipeline.py took 22.816 seconds
[2025-05-16T00:06:02.103+0000] {processor.py:161} INFO - Started process (PID=495) to work on /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:06:03.804+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/breweries_pipeline.py for tasks to queue
[2025-05-16T00:06:03.818+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:06:03.816+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:06:05.588+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/bronze_path = s3a://bronze
[2025-05-16T00:06:05.637+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/silver_path = s3a://silver
[2025-05-16T00:06:05.713+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/gold_path = s3a://gold
[2025-05-16T00:06:05.733+0000] {processor.py:840} INFO - DAG(s) 'breweries_etl_pipeline' retrieved from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:06:09.634+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:06:09.633+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-05-16T00:06:09.688+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:06:09.687+0000] {dag.py:3823} INFO - Setting next_dagrun for breweries_etl_pipeline to 2025-05-16 00:00:00+00:00, run_after=2025-05-17 00:00:00+00:00
[2025-05-16T00:07:22.535+0000] {processor.py:161} INFO - Started process (PID=497) to work on /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:07:22.539+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/breweries_pipeline.py for tasks to queue
[2025-05-16T00:07:22.543+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:07:22.542+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:07:24.245+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/bronze_path = s3a://bronze
[2025-05-16T00:07:24.279+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/silver_path = s3a://silver
[2025-05-16T00:07:24.313+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/gold_path = s3a://gold
[2025-05-16T00:07:24.346+0000] {processor.py:840} INFO - DAG(s) 'breweries_etl_pipeline' retrieved from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:07:24.698+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:07:24.697+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-05-16T00:07:28.780+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:07:28.780+0000] {dag.py:3823} INFO - Setting next_dagrun for breweries_etl_pipeline to 2025-05-16 00:00:00+00:00, run_after=2025-05-17 00:00:00+00:00
[2025-05-16T00:07:29.206+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:07:29.197+0000] {dagbag.py:647} ERROR - Failed to write serialized DAG: /opt/airflow/dags/breweries_pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 635, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 157, in write_dag
    if session.scalar(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(breweries_etl_pipeline) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'breweries_etl_pipeline', 'fileloc': '/opt/airflow/dags/breweries_pipeline.py', 'fileloc_hash': 1458171675363101, 'data': '{"__version": 1, "dag": {"_dag_id": "breweries_etl_pipeline", "fileloc": "/opt/airflow/dags/breweries_pipeline.py", "_description": "Pipeline ETL com ... (9812 characters truncated) ... hon", "_is_empty": false, "op_args": [], "op_kwargs": {"execution_date": "{{ ds | replace(\'-\', \'\') }}"}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2025, 5, 16, 0, 7, 24, 375514, tzinfo=Timezone('UTC')), 'dag_hash': '17f92dda176ba27ad464420ff78ef2f7', 'processor_subdir': '/opt/airflow/dags'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2025-05-16T00:07:29.208+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:07:29.208+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-05-16T00:07:29.211+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 657, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 673, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 3048, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(breweries_etl_pipeline) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'breweries_etl_pipeline', 'fileloc': '/opt/airflow/dags/breweries_pipeline.py', 'fileloc_hash': 1458171675363101, 'data': '{"__version": 1, "dag": {"_dag_id": "breweries_etl_pipeline", "fileloc": "/opt/airflow/dags/breweries_pipeline.py", "_description": "Pipeline ETL com ... (9812 characters truncated) ... hon", "_is_empty": false, "op_args": [], "op_kwargs": {"execution_date": "{{ ds | replace(\'-\', \'\') }}"}}], "dag_dependencies": [], "params": {}}}', 'data_compressed': None, 'last_updated': datetime.datetime(2025, 5, 16, 0, 7, 24, 375514, tzinfo=Timezone('UTC')), 'dag_hash': '17f92dda176ba27ad464420ff78ef2f7', 'processor_subdir': '/opt/airflow/dags'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2025-05-16T00:07:59.582+0000] {processor.py:161} INFO - Started process (PID=499) to work on /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:07:59.586+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/breweries_pipeline.py for tasks to queue
[2025-05-16T00:07:59.589+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:07:59.588+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:08:15.069+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/bronze_path = s3a://bronze
[2025-05-16T00:08:15.107+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/silver_path = s3a://silver
[2025-05-16T00:08:15.142+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/gold_path = s3a://gold
[2025-05-16T00:08:16.270+0000] {processor.py:840} INFO - DAG(s) 'breweries_etl_pipeline' retrieved from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:08:28.986+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:08:28.984+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-05-16T00:08:29.046+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:08:29.045+0000] {dag.py:3823} INFO - Setting next_dagrun for breweries_etl_pipeline to 2025-05-16 00:00:00+00:00, run_after=2025-05-17 00:00:00+00:00
[2025-05-16T00:08:29.124+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/breweries_pipeline.py took 29.552 seconds
[2025-05-16T00:08:59.238+0000] {processor.py:161} INFO - Started process (PID=501) to work on /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:08:59.241+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/breweries_pipeline.py for tasks to queue
[2025-05-16T00:08:59.244+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:08:59.243+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:09:00.947+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/bronze_path = s3a://bronze
[2025-05-16T00:09:00.965+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/silver_path = s3a://silver
[2025-05-16T00:09:00.980+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/gold_path = s3a://gold
[2025-05-16T00:09:01.007+0000] {processor.py:840} INFO - DAG(s) 'breweries_etl_pipeline' retrieved from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:09:01.087+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:09:01.086+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-05-16T00:09:01.160+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:09:01.159+0000] {dag.py:3823} INFO - Setting next_dagrun for breweries_etl_pipeline to 2025-05-16 00:00:00+00:00, run_after=2025-05-17 00:00:00+00:00
[2025-05-16T00:09:01.214+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/breweries_pipeline.py took 1.988 seconds
[2025-05-16T00:10:42.374+0000] {processor.py:161} INFO - Started process (PID=29) to work on /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:10:42.378+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/breweries_pipeline.py for tasks to queue
[2025-05-16T00:10:42.381+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:10:42.381+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:10:43.944+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/bronze_path = s3a://bronze
[2025-05-16T00:10:43.968+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/silver_path = s3a://silver
[2025-05-16T00:10:43.996+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/gold_path = s3a://gold
[2025-05-16T00:10:44.016+0000] {processor.py:840} INFO - DAG(s) 'breweries_etl_pipeline' retrieved from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:10:44.195+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:10:44.195+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-05-16T00:10:44.264+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:10:44.263+0000] {dag.py:3823} INFO - Setting next_dagrun for breweries_etl_pipeline to 2025-05-16 00:00:00+00:00, run_after=2025-05-17 00:00:00+00:00
[2025-05-16T00:10:44.304+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/breweries_pipeline.py took 1.937 seconds
[2025-05-16T00:11:14.609+0000] {processor.py:161} INFO - Started process (PID=35) to work on /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:11:14.610+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/breweries_pipeline.py for tasks to queue
[2025-05-16T00:11:14.613+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:11:14.613+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:11:15.927+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/bronze_path = s3a://bronze
[2025-05-16T00:11:15.943+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/silver_path = s3a://silver
[2025-05-16T00:11:15.962+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/gold_path = s3a://gold
[2025-05-16T00:11:15.983+0000] {processor.py:840} INFO - DAG(s) 'breweries_etl_pipeline' retrieved from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:11:16.088+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:11:16.086+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-05-16T00:11:27.729+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:11:27.729+0000] {dag.py:3823} INFO - Setting next_dagrun for breweries_etl_pipeline to 2025-05-16 00:00:00+00:00, run_after=2025-05-17 00:00:00+00:00
[2025-05-16T00:12:35.673+0000] {processor.py:161} INFO - Started process (PID=37) to work on /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:12:36.513+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/breweries_pipeline.py for tasks to queue
[2025-05-16T00:12:36.517+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:12:36.517+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:13:07.896+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:13:07.895+0000] {timeout.py:68} ERROR - Process timed out, PID: 37
[2025-05-16T00:13:17.688+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:13:07.897+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/breweries_pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/breweries_pipeline.py", line 5, in <module>
    from src.extract import extract_breweries
  File "/opt/airflow/dags/src/extract.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/__init__.py", line 22, in <module>
    from pandas.compat import is_numpy_dev as _is_numpy_dev  # pyright: ignore # noqa:F401
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/compat/__init__.py", line 25, in <module>
    from pandas.compat.numpy import (
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/compat/numpy/__init__.py", line 4, in <module>
    from pandas.util.version import Version
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/util/__init__.py", line 2, in <module>
    from pandas.util._decorators import (  # noqa:F401
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/util/_decorators.py", line 14, in <module>
    from pandas._libs.properties import cache_readonly
  File "<frozen importlib._bootstrap>", line 991, in _find_and_load
  File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 839, in exec_module
  File "<frozen importlib._bootstrap_external>", line 934, in get_code
  File "<frozen importlib._bootstrap_external>", line 1033, in get_data
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/breweries_pipeline.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 37
[2025-05-16T00:13:17.690+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:13:19.051+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/breweries_pipeline.py took 43.388 seconds
[2025-05-16T00:13:49.346+0000] {processor.py:161} INFO - Started process (PID=38) to work on /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:13:49.347+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/breweries_pipeline.py for tasks to queue
[2025-05-16T00:13:49.349+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:13:49.348+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:14:21.826+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:14:21.822+0000] {timeout.py:68} ERROR - Process timed out, PID: 38
[2025-05-16T00:14:23.461+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:14:22.134+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/breweries_pipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/breweries_pipeline.py", line 5, in <module>
    from src.extract import extract_breweries
  File "/opt/airflow/dags/src/extract.py", line 3, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/__init__.py", line 22, in <module>
    from pandas.compat import is_numpy_dev as _is_numpy_dev  # pyright: ignore # noqa:F401
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/compat/__init__.py", line 29, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.8/site-packages/pyarrow/__init__.py", line 274, in <module>
    import pyarrow.hdfs as hdfs
  File "/home/airflow/.local/lib/python3.8/site-packages/pyarrow/hdfs.py", line 25, in <module>
    from pyarrow.filesystem import FileSystem
  File "<frozen importlib._bootstrap>", line 991, in _find_and_load
  File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 839, in exec_module
  File "<frozen importlib._bootstrap_external>", line 934, in get_code
  File "<frozen importlib._bootstrap_external>", line 1033, in get_data
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/breweries_pipeline.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 38
[2025-05-16T00:14:23.464+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:14:37.067+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/breweries_pipeline.py took 47.727 seconds
[2025-05-16T00:15:07.555+0000] {processor.py:161} INFO - Started process (PID=40) to work on /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:15:08.225+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/breweries_pipeline.py for tasks to queue
[2025-05-16T00:15:08.982+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:15:08.981+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:15:20.626+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/bronze_path = s3a://bronze
[2025-05-16T00:15:20.655+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/silver_path = s3a://silver
[2025-05-16T00:15:20.677+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/gold_path = s3a://gold
[2025-05-16T00:15:20.729+0000] {processor.py:840} INFO - DAG(s) 'breweries_etl_pipeline' retrieved from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:15:21.611+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:15:21.609+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-05-16T00:15:21.660+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:15:21.660+0000] {dag.py:3823} INFO - Setting next_dagrun for breweries_etl_pipeline to 2025-05-16 00:00:00+00:00, run_after=2025-05-17 00:00:00+00:00
[2025-05-16T00:15:21.706+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/breweries_pipeline.py took 14.162 seconds
[2025-05-16T00:15:52.316+0000] {processor.py:161} INFO - Started process (PID=46) to work on /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:15:52.319+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/breweries_pipeline.py for tasks to queue
[2025-05-16T00:15:52.321+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:15:52.321+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:15:53.957+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/bronze_path = s3a://bronze
[2025-05-16T00:15:53.995+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/silver_path = s3a://silver
[2025-05-16T00:15:54.031+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/gold_path = s3a://gold
[2025-05-16T00:15:54.060+0000] {processor.py:840} INFO - DAG(s) 'breweries_etl_pipeline' retrieved from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:15:54.109+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:15:54.109+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-05-16T00:15:54.140+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:15:54.140+0000] {dag.py:3823} INFO - Setting next_dagrun for breweries_etl_pipeline to 2025-05-16 00:00:00+00:00, run_after=2025-05-17 00:00:00+00:00
[2025-05-16T00:15:56.552+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/breweries_pipeline.py took 4.246 seconds
[2025-05-16T00:16:26.621+0000] {processor.py:161} INFO - Started process (PID=48) to work on /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:16:27.109+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/breweries_pipeline.py for tasks to queue
[2025-05-16T00:16:27.112+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:16:27.111+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:16:28.852+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/bronze_path = s3a://bronze
[2025-05-16T00:16:28.890+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/silver_path = s3a://silver
[2025-05-16T00:16:28.926+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/gold_path = s3a://gold
[2025-05-16T00:16:28.955+0000] {processor.py:840} INFO - DAG(s) 'breweries_etl_pipeline' retrieved from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:16:29.035+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:16:29.034+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-05-16T00:16:30.285+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:16:30.284+0000] {dag.py:3823} INFO - Setting next_dagrun for breweries_etl_pipeline to 2025-05-16 00:00:00+00:00, run_after=2025-05-17 00:00:00+00:00
[2025-05-16T00:16:30.742+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/breweries_pipeline.py took 4.132 seconds
[2025-05-16T00:17:00.924+0000] {processor.py:161} INFO - Started process (PID=50) to work on /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:17:00.958+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/breweries_pipeline.py for tasks to queue
[2025-05-16T00:17:00.996+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:17:00.995+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:17:02.823+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/bronze_path = s3a://bronze
[2025-05-16T00:17:02.864+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/silver_path = s3a://silver
[2025-05-16T00:17:02.889+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/gold_path = s3a://gold
[2025-05-16T00:17:02.915+0000] {processor.py:840} INFO - DAG(s) 'breweries_etl_pipeline' retrieved from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:17:03.038+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:17:03.037+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-05-16T00:17:03.118+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:17:03.118+0000] {dag.py:3823} INFO - Setting next_dagrun for breweries_etl_pipeline to 2025-05-16 00:00:00+00:00, run_after=2025-05-17 00:00:00+00:00
[2025-05-16T00:17:03.314+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/breweries_pipeline.py took 2.402 seconds
[2025-05-16T00:17:33.986+0000] {processor.py:161} INFO - Started process (PID=52) to work on /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:17:33.989+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/breweries_pipeline.py for tasks to queue
[2025-05-16T00:17:33.992+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:17:33.991+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:17:35.551+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/bronze_path = s3a://bronze
[2025-05-16T00:17:36.013+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/silver_path = s3a://silver
[2025-05-16T00:17:36.054+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/gold_path = s3a://gold
[2025-05-16T00:17:36.085+0000] {processor.py:840} INFO - DAG(s) 'breweries_etl_pipeline' retrieved from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:17:36.176+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:17:36.175+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-05-16T00:17:36.240+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:17:36.239+0000] {dag.py:3823} INFO - Setting next_dagrun for breweries_etl_pipeline to 2025-05-16 00:00:00+00:00, run_after=2025-05-17 00:00:00+00:00
[2025-05-16T00:18:55.150+0000] {processor.py:161} INFO - Started process (PID=54) to work on /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:18:55.590+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/breweries_pipeline.py for tasks to queue
[2025-05-16T00:18:55.592+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:18:55.592+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:18:57.757+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/bronze_path = s3a://bronze
[2025-05-16T00:18:57.957+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/silver_path = s3a://silver
[2025-05-16T00:18:59.177+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/gold_path = s3a://gold
[2025-05-16T00:18:59.872+0000] {processor.py:840} INFO - DAG(s) 'breweries_etl_pipeline' retrieved from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:19:00.300+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:19:00.299+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-05-16T00:21:03.292+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:21:03.292+0000] {dag.py:3823} INFO - Setting next_dagrun for breweries_etl_pipeline to 2025-05-16 00:00:00+00:00, run_after=2025-05-17 00:00:00+00:00
[2025-05-16T00:21:05.342+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/breweries_pipeline.py took 130.202 seconds
[2025-05-16T00:21:36.996+0000] {processor.py:161} INFO - Started process (PID=56) to work on /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:21:37.965+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/breweries_pipeline.py for tasks to queue
[2025-05-16T00:21:37.969+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:21:37.968+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:21:39.619+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/bronze_path = s3a://bronze
[2025-05-16T00:21:41.246+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/silver_path = s3a://silver
[2025-05-16T00:21:41.265+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/gold_path = s3a://gold
[2025-05-16T00:21:41.293+0000] {processor.py:840} INFO - DAG(s) 'breweries_etl_pipeline' retrieved from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:21:41.632+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:21:41.631+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-05-16T00:21:42.636+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:21:42.635+0000] {dag.py:3823} INFO - Setting next_dagrun for breweries_etl_pipeline to 2025-05-16 00:00:00+00:00, run_after=2025-05-17 00:00:00+00:00
[2025-05-16T00:21:43.131+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/breweries_pipeline.py took 6.145 seconds
[2025-05-16T00:22:13.735+0000] {processor.py:161} INFO - Started process (PID=58) to work on /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:22:13.738+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/breweries_pipeline.py for tasks to queue
[2025-05-16T00:22:13.742+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:22:13.740+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:22:15.407+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/bronze_path = s3a://bronze
[2025-05-16T00:22:15.443+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/silver_path = s3a://silver
[2025-05-16T00:22:15.480+0000] {logging_mixin.py:188} INFO - ✔️ /datalake/gold_path = s3a://gold
[2025-05-16T00:22:15.510+0000] {processor.py:840} INFO - DAG(s) 'breweries_etl_pipeline' retrieved from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T00:22:15.602+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:22:15.601+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-05-16T00:22:15.664+0000] {logging_mixin.py:188} INFO - [2025-05-16T00:22:15.663+0000] {dag.py:3823} INFO - Setting next_dagrun for breweries_etl_pipeline to 2025-05-16 00:00:00+00:00, run_after=2025-05-17 00:00:00+00:00
[2025-05-16T00:22:16.902+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/breweries_pipeline.py took 3.177 seconds
[2025-05-16T09:02:51.359+0000] {processor.py:161} INFO - Started process (PID=33) to work on /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T09:02:51.361+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/breweries_pipeline.py for tasks to queue
[2025-05-16T09:02:51.366+0000] {logging_mixin.py:188} INFO - [2025-05-16T09:02:51.365+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T09:03:03.701+0000] {logging_mixin.py:188} INFO - ⚠️ Erro ao obter /datalake/bronze_path do SSM: Could not connect to the endpoint URL: "http://localstack:4566/"
[2025-05-16T09:03:03.703+0000] {logging_mixin.py:188} INFO - ➡️ Usando valor padrão: s3a://bronze
[2025-05-16T09:03:10.495+0000] {logging_mixin.py:188} INFO - ⚠️ Erro ao obter /datalake/silver_path do SSM: Could not connect to the endpoint URL: "http://localstack:4566/"
[2025-05-16T09:03:10.502+0000] {logging_mixin.py:188} INFO - ➡️ Usando valor padrão: s3a://silver
[2025-05-16T09:03:18.704+0000] {logging_mixin.py:188} INFO - ⚠️ Erro ao obter /datalake/gold_path do SSM: Could not connect to the endpoint URL: "http://localstack:4566/"
[2025-05-16T09:03:18.707+0000] {logging_mixin.py:188} INFO - ➡️ Usando valor padrão: s3a://gold
[2025-05-16T09:03:18.761+0000] {processor.py:840} INFO - DAG(s) 'breweries_etl_pipeline' retrieved from /opt/airflow/dags/breweries_pipeline.py
[2025-05-16T09:03:18.992+0000] {logging_mixin.py:188} INFO - [2025-05-16T09:03:18.992+0000] {override.py:1769} INFO - Created Permission View: can edit on DAG:breweries_etl_pipeline
[2025-05-16T09:03:19.001+0000] {logging_mixin.py:188} INFO - [2025-05-16T09:03:19.000+0000] {override.py:1769} INFO - Created Permission View: can read on DAG:breweries_etl_pipeline
[2025-05-16T09:03:19.006+0000] {logging_mixin.py:188} INFO - [2025-05-16T09:03:19.006+0000] {override.py:1769} INFO - Created Permission View: can delete on DAG:breweries_etl_pipeline
[2025-05-16T09:03:19.006+0000] {logging_mixin.py:188} INFO - [2025-05-16T09:03:19.006+0000] {dag.py:3036} INFO - Sync 1 DAGs
[2025-05-16T09:03:19.017+0000] {logging_mixin.py:188} INFO - [2025-05-16T09:03:19.017+0000] {dag.py:3058} INFO - Creating ORM DAG for breweries_etl_pipeline
[2025-05-16T09:03:19.029+0000] {logging_mixin.py:188} INFO - [2025-05-16T09:03:19.028+0000] {dag.py:3823} INFO - Setting next_dagrun for breweries_etl_pipeline to 2025-05-15 00:00:00+00:00, run_after=2025-05-16 00:00:00+00:00
[2025-05-16T09:03:19.045+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/breweries_pipeline.py took 27.691 seconds
